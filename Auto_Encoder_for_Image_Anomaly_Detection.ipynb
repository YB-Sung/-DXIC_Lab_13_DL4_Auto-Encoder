{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YB-Sung/DXIC_Lab_13_DL4_Auto-Encoder/blob/main/Auto_Encoder_for_Image_Anomaly_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [ LG전자 H&A DX Intensive Course - Auto-Encoder for Anomaly Detection ]\n",
        "\n",
        "Auto-Encoder를 활용한 image anomaly detection"
      ],
      "metadata": {
        "id": "wWMZM-0nOd1L"
      },
      "id": "wWMZM-0nOd1L"
    },
    {
      "cell_type": "markdown",
      "id": "996a8fa0",
      "metadata": {
        "id": "996a8fa0"
      },
      "source": [
        "# Import modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbd981c5",
      "metadata": {
        "id": "cbd981c5"
      },
      "outputs": [],
      "source": [
        "from glob import glob\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import cv2\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import SGD, Adam\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "custom_params = {\"axes.spines.right\": False, \"axes.spines.top\": False}\n",
        "sns.set_theme(style=\"ticks\", rc=custom_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95fa0fb1",
      "metadata": {
        "id": "95fa0fb1"
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7317bd2d",
      "metadata": {
        "id": "7317bd2d"
      },
      "outputs": [],
      "source": [
        "def torch_seed(random_seed):\n",
        "    torch.manual_seed(random_seed)\n",
        "    torch.cuda.manual_seed(random_seed)\n",
        "    torch.cuda.manual_seed_all(random_seed) # if use multi-GPU\n",
        "    # CUDA randomness\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    np.random.seed(random_seed)\n",
        "    random.seed(random_seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(random_seed)\n",
        "\n",
        "\n",
        "def train(\n",
        "    model, dataloader, criterion, optimizer, log_interval: int, device: str) -> list:\n",
        "\n",
        "    total_loss = []\n",
        "\n",
        "    model.train()\n",
        "    for i, (inputs, _, _) in enumerate(dataloader):\n",
        "\n",
        "        # convert device\n",
        "        inputs = inputs.to(device)\n",
        "\n",
        "        # model outputs\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # loss\n",
        "        loss = criterion(inputs, outputs).mean()\n",
        "        total_loss.append(loss.item())\n",
        "\n",
        "        # calculate gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # update model weights\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # log learning history\n",
        "        if i % log_interval == 0 or (i+1) == len(dataloader):\n",
        "            print(f\"{'TRAIN':5s} [{i+1:5d}/{len(dataloader):5d}] loss: {np.mean(total_loss):.4f}\")\n",
        "\n",
        "    # average loss\n",
        "    avg_loss = np.mean(total_loss)\n",
        "\n",
        "    return avg_loss\n",
        "\n",
        "def test(\n",
        "    model, dataloader, criterion, log_interval: int, device: str) -> list:\n",
        "\n",
        "    # for image-level auroc\n",
        "    total_loss_img = []\n",
        "    total_targets = []\n",
        "    # for pixel-level auroc\n",
        "    total_masks = []\n",
        "    total_loss_pixel = []\n",
        "\n",
        "    torch_seed(223)\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, masks, targets) in enumerate(dataloader):\n",
        "            # get masks\n",
        "            total_masks.append(masks.numpy())\n",
        "\n",
        "            # get targets\n",
        "            total_targets.extend(targets.tolist())\n",
        "\n",
        "            # convert device\n",
        "            inputs = inputs.to(device)\n",
        "\n",
        "            # model outputs\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # loss\n",
        "            loss = criterion(inputs, outputs)\n",
        "            total_loss_img.extend(loss.flatten(start_dim=1).max(dim=1)[0].cpu().tolist())\n",
        "            total_loss_pixel.append(loss.max(dim=1)[0].cpu().numpy())\n",
        "\n",
        "            # log learning history\n",
        "            if i % log_interval == 0 or (i+1) == len(dataloader):\n",
        "                print(f\"{'TSET':5s} [{i+1:5d}/{len(dataloader):5d}] loss: {np.mean(total_loss_img):.4f}\")\n",
        "\n",
        "    # image-level auroc\n",
        "    auroc_img = roc_auc_score(total_targets, total_loss_img)\n",
        "\n",
        "    # pixel-level auroc\n",
        "    total_loss_pixel = np.vstack(total_loss_pixel).reshape(-1)\n",
        "    total_masks = np.vstack(total_masks).reshape(-1)\n",
        "    auroc_pixel = roc_auc_score(total_masks, total_loss_pixel)\n",
        "\n",
        "    # return\n",
        "    return auroc_img, auroc_pixel\n",
        "\n",
        "\n",
        "def fit(\n",
        "    model, trainloader, testloader, criterion, optimizer,\n",
        "    epochs: int, log_interval: int, device: str) -> list:\n",
        "\n",
        "    train_history = []\n",
        "    test_history_auroc_img = []\n",
        "    test_history_auroc_pixel = []\n",
        "\n",
        "    # fitting model\n",
        "    for i in range(epochs):\n",
        "        print(f'\\nEpoch: [{i+1}/{epochs}]')\n",
        "        train_loss = train(\n",
        "            model        = model,\n",
        "            dataloader   = trainloader,\n",
        "            criterion    = criterion,\n",
        "            optimizer    = optimizer,\n",
        "            log_interval = log_interval,\n",
        "            device       = device\n",
        "        )\n",
        "\n",
        "        test_auroc_img, test_auroc_pixel = test(\n",
        "            model        = model,\n",
        "            dataloader   = testloader,\n",
        "            criterion    = criterion,\n",
        "            log_interval = log_interval,\n",
        "            device       = device\n",
        "        )\n",
        "\n",
        "        print(f'\\nTest AUROC-image: {test_auroc_img:.4f}, AUROC-pixel: {test_auroc_pixel:.4f}')\n",
        "\n",
        "        # show results\n",
        "        with torch.no_grad():\n",
        "            test_category = testloader.dataset.category\n",
        "            fig, ax = plt.subplots(2, len(test_category), figsize=(2*len(test_category), 5))\n",
        "\n",
        "            file_list_cat = list(map(lambda x: x.split('/')[-2], testloader.dataset.file_list))\n",
        "\n",
        "            for i, c in enumerate(test_category):\n",
        "                # select image per category\n",
        "                idx = np.where(np.array(file_list_cat) == c)[0][0]\n",
        "                img, mask, _ = testset[idx]\n",
        "\n",
        "                # inference\n",
        "                output = model(img.unsqueeze(0).to(device)).cpu()[0]\n",
        "\n",
        "                # show image\n",
        "                ax[0, i].imshow(img.permute(1,2,0))\n",
        "                ax[1, i].imshow(output.permute(1,2,0))\n",
        "\n",
        "                # axis off\n",
        "                ax[0, i].axis('off')\n",
        "                ax[1, i].axis('off')\n",
        "\n",
        "                # set title\n",
        "                ax[0, i].set_title(f\"{c}\\nimage\")\n",
        "                ax[1, i].set_title(f\"{c}\\nreconstruction\")\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "        # stack history\n",
        "        train_history.append(train_loss)\n",
        "        test_history_auroc_img.append(test_auroc_img)\n",
        "        test_history_auroc_pixel.append(test_auroc_pixel)\n",
        "\n",
        "    return train_history, test_history_auroc_img, test_history_auroc_pixel\n",
        "\n",
        "\n",
        "def figure(\n",
        "    all_train_history: list, all_test_history_auroc_img: list,\n",
        "    all_test_history_auroc_pixel: list, all_exp_name: list) -> None:\n",
        "\n",
        "    fig, ax = plt.subplots(1, 3, figsize=(15,5))\n",
        "\n",
        "    # train line plot\n",
        "    for i, (train_h, exp_name) in enumerate(zip(all_train_history, all_exp_name)):\n",
        "        sns.lineplot(\n",
        "            x     = range(1, len(train_h)+1),\n",
        "            y     = train_h,\n",
        "            label = exp_name,\n",
        "            ax    = ax[0]\n",
        "        )\n",
        "\n",
        "    # test image-level AUROC lineplot\n",
        "    for i, (test_h, exp_name) in enumerate(zip(all_test_history_auroc_img, all_exp_name)):\n",
        "        sns.lineplot(\n",
        "            x     = range(1, len(test_h)+1),\n",
        "            y     = test_h,\n",
        "            label = exp_name,\n",
        "            ax    = ax[1]\n",
        "        )\n",
        "\n",
        "    # test pixel-level AUROC lineplot\n",
        "    for i, (test_h, exp_name) in enumerate(zip(all_test_history_auroc_pixel, all_exp_name)):\n",
        "        sns.lineplot(\n",
        "            x     = range(1, len(test_h)+1),\n",
        "            y     = test_h,\n",
        "            label = exp_name,\n",
        "            ax    = ax[2]\n",
        "        )\n",
        "\n",
        "    # set y axis label\n",
        "    ax[0].set_ylabel('MSE Loss')\n",
        "    ax[1].set_ylabel('AUROC(image-level)')\n",
        "    ax[2].set_ylabel('AUROC(pixel-level)')\n",
        "\n",
        "    # set x axis label\n",
        "    ax[0].set_xlabel('Epochs')\n",
        "    ax[1].set_xlabel('Epochs')\n",
        "    ax[2].set_xlabel('Epochs')\n",
        "\n",
        "    # set title\n",
        "    ax[0].set_title('Train loss history')\n",
        "    ax[1].set_title('Test AUROC(image-level) history')\n",
        "    ax[2].set_title('Test AUROC(pixel-level) history')\n",
        "\n",
        "    # set y value limit\n",
        "    max_train = np.max(all_train_history)\n",
        "\n",
        "    ax[0].set_ylim(0, max_train+0.01)\n",
        "    ax[1].set_ylim(0, 1)\n",
        "    ax[2].set_ylim(0, 1)\n",
        "\n",
        "    # set legend\n",
        "    ax[0].legend(loc='upper left')\n",
        "    ax[1].legend(loc='upper left')\n",
        "    ax[2].legend(loc='upper left')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82ec5b5f",
      "metadata": {
        "id": "82ec5b5f"
      },
      "source": [
        "# Configuration for experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2fbfab9",
      "metadata": {
        "id": "a2fbfab9"
      },
      "outputs": [],
      "source": [
        "class Config:\n",
        "    # dataset 관련 parameters\n",
        "    datadir = './data'\n",
        "    target = 'bottle'\n",
        "    image_size = [224, 224]\n",
        "\n",
        "    # training 관련 parameters\n",
        "    epochs = 20\n",
        "    batch_size = 8\n",
        "    test_batch_size = 128\n",
        "    learning_rate = 0.01\n",
        "    num_workers = 2\n",
        "    log_interval = 2000\n",
        "\n",
        "    # device\n",
        "    device = 'cuda'\n",
        "\n",
        "    # seed\n",
        "    seed = 223\n",
        "\n",
        "cfg = Config()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "024ec5b1",
      "metadata": {
        "id": "024ec5b1"
      },
      "source": [
        "# Load dataset and dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0c9b0c1",
      "metadata": {
        "id": "e0c9b0c1"
      },
      "source": [
        "**Download data**\n",
        "- MVTec AD [ [link](https://www.mvtec.com/company/research/datasets/mvtec-ad) ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "908621a1",
      "metadata": {
        "id": "908621a1"
      },
      "outputs": [],
      "source": [
        "!wget -P './data' 'https://www.mydrive.ch/shares/38536/3830184030e49fe74747669442f0f282/download/420937370-1629951468/bottle.tar.xz'\n",
        "!tar Jxvf ./data/bottle.tar.xz -C ./data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae76c8b5",
      "metadata": {
        "id": "ae76c8b5"
      },
      "source": [
        "```bash\n",
        "./data/bottle\n",
        "├── ground_truth\n",
        "│   ├── broken_large\n",
        "│   ├── broken_small\n",
        "│   └── contamination\n",
        "├── license.txt\n",
        "├── readme.txt\n",
        "├── test\n",
        "│   ├── broken_large\n",
        "│   ├── broken_small\n",
        "│   ├── contamination\n",
        "│   └── good\n",
        "└── train\n",
        "    └── good\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3804927",
      "metadata": {
        "id": "f3804927"
      },
      "outputs": [],
      "source": [
        "print('[ trainset ]')\n",
        "print(f\"train good images: {len(glob(os.path.join(cfg.datadir, cfg.target, 'train/good/*')))}\")\n",
        "print('\\n[ testset ]')\n",
        "testdir = os.path.join(cfg.datadir, cfg.target, 'test')\n",
        "for name in os.listdir(testdir):\n",
        "    print(f\"test {name} images: {len(glob(os.path.join(testdir, name, '*')))}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "492739d4",
      "metadata": {
        "id": "492739d4"
      },
      "outputs": [],
      "source": [
        "class MVTecAD(Dataset):\n",
        "    def __init__(\n",
        "        self, datadir: str, target: str, train: bool,\n",
        "        img_size: list, transform: transforms.Compose):\n",
        "\n",
        "        self.datadir = os.path.join(datadir, target)\n",
        "        self.train = train\n",
        "\n",
        "        self.category = os.listdir(os.path.join(self.datadir, 'train' if train else 'test'))\n",
        "\n",
        "        self.img_size = img_size\n",
        "        self.transform = transform\n",
        "\n",
        "        self.file_list = glob(os.path.join(self.datadir, 'train' if self.train else 'test', '*/*'))\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file_path = self.file_list[idx]\n",
        "\n",
        "        # image\n",
        "        img = cv2.imread(file_path)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        img = cv2.resize(img, dsize=self.img_size)\n",
        "\n",
        "        # target\n",
        "        target = 0 if 'good' in file_path else 1\n",
        "\n",
        "        # mask\n",
        "        if 'good' in file_path:\n",
        "            mask = np.zeros(self.img_size, dtype=np.float32)\n",
        "        else:\n",
        "            mask = cv2.imread(\n",
        "                file_path.replace('test','ground_truth').replace('.png','_mask.png'),\n",
        "                cv2.IMREAD_GRAYSCALE\n",
        "            )\n",
        "            mask = cv2.resize(mask, dsize=self.img_size).astype(bool).astype(int)\n",
        "\n",
        "        img = self.transform(img)\n",
        "        mask = torch.Tensor(mask).to(torch.int64)\n",
        "\n",
        "        return img, mask, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69214ac0",
      "metadata": {
        "id": "69214ac0"
      },
      "outputs": [],
      "source": [
        "# define dataset and dataloader\n",
        "trainset = MVTecAD(\n",
        "    datadir   = cfg.datadir,\n",
        "    target    = cfg.target,\n",
        "    img_size  = cfg.image_size,\n",
        "    transform = transforms.ToTensor(),\n",
        "    train     = True\n",
        ")\n",
        "\n",
        "testset = MVTecAD(\n",
        "    datadir   = cfg.datadir,\n",
        "    target    = cfg.target,\n",
        "    img_size  = cfg.image_size,\n",
        "    transform = transforms.ToTensor(),\n",
        "    train     = False\n",
        ")\n",
        "\n",
        "trainloader = DataLoader(trainset, batch_size=cfg.batch_size, shuffle=True, num_workers=cfg.num_workers)\n",
        "testloader = DataLoader(testset, batch_size=cfg.test_batch_size, shuffle=False, num_workers=cfg.num_workers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39a990f3",
      "metadata": {
        "id": "39a990f3"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(2, len(testset.category), figsize=(2*len(testset.category), 5))\n",
        "\n",
        "file_list_cat = list(map(lambda x: x.split('/')[-2], testset.file_list))\n",
        "\n",
        "for i, c in enumerate(testset.category):\n",
        "    idx = np.where(np.array(file_list_cat) == c)[0][0]\n",
        "    img, mask, _ = testset[idx]\n",
        "    ax[0, i].imshow(img.permute(1,2,0))\n",
        "    ax[1, i].imshow(mask, cmap='gray')\n",
        "\n",
        "    # axis off\n",
        "    ax[0, i].axis('off')\n",
        "    ax[1, i].axis('off')\n",
        "\n",
        "    # set title\n",
        "    ax[0, i].set_title(c)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f828e52b",
      "metadata": {
        "id": "f828e52b"
      },
      "source": [
        "# Convolutional Auto-Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b031dbbc",
      "metadata": {
        "id": "b031dbbc"
      },
      "outputs": [],
      "source": [
        "class ConvolutionalAutoEncoder(nn.Module):\n",
        "    def __init__(self, input_dim: int, dims: list):\n",
        "        super().__init__()\n",
        "\n",
        "        dims = [input_dim] + dims\n",
        "\n",
        "        self.enc = nn.Sequential(*self.build_layer(dims=dims))\n",
        "        self.dec = nn.Sequential(*self.build_layer(dims=dims[::-1], up=True))\n",
        "        self.output = nn.Conv2d(\n",
        "            in_channels  = input_dim,\n",
        "            out_channels = input_dim,\n",
        "            kernel_size  = 3,\n",
        "            padding      = 1\n",
        "        )\n",
        "\n",
        "    def build_layer(self, dims, up=False):\n",
        "        layer = []\n",
        "\n",
        "        for i in range(1, len(dims)):\n",
        "            if up:\n",
        "                layer_i = [\n",
        "                    nn.ConvTranspose2d(\n",
        "                        in_channels  = dims[i-1],\n",
        "                        out_channels = dims[i],\n",
        "                        kernel_size  = 2,\n",
        "                        stride       = 2\n",
        "                    ),\n",
        "                    nn.ReLU()\n",
        "                ]\n",
        "            else:\n",
        "                layer_i = [\n",
        "                    nn.Conv2d(\n",
        "                        in_channels  = dims[i-1],\n",
        "                        out_channels = dims[i],\n",
        "                        kernel_size  = 3,\n",
        "                        padding      = 1),\n",
        "                    nn.ReLU(),\n",
        "                    nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "                ]\n",
        "\n",
        "            layer.extend(layer_i)\n",
        "\n",
        "        return layer\n",
        "\n",
        "    def encoder(self, x):\n",
        "        out = self.enc(x)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def decoder(self, out):\n",
        "        out = self.dec(out)\n",
        "        out = self.output(out)\n",
        "        out = F.sigmoid(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.encoder(x)\n",
        "        out = self.decoder(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7360cff",
      "metadata": {
        "id": "a7360cff"
      },
      "source": [
        "## CAE - shallow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afee69fb",
      "metadata": {
        "id": "afee69fb"
      },
      "outputs": [],
      "source": [
        "torch_seed(cfg.seed)\n",
        "cae_shallow = ConvolutionalAutoEncoder(input_dim=3, dims=[32, 64])\n",
        "cae_shallow.to(cfg.device)\n",
        "print('load Convolutional Auto-Encoder')\n",
        "print('The number of model parameters: ',sum([p.numel() for p in cae_shallow.parameters()]))\n",
        "\n",
        "# set reduction to none\n",
        "criterion = nn.MSELoss(reduction='none')\n",
        "optimizer = Adam(cae_shallow.parameters(), lr=cfg.learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75eb7048",
      "metadata": {
        "scrolled": false,
        "id": "75eb7048"
      },
      "outputs": [],
      "source": [
        "torch_seed(cfg.seed)\n",
        "train_history_cae_shallow, test_history_auroc_img_cae_shallow, test_history_auroc_pixel_cae_shallow = fit(\n",
        "    model        = cae_shallow,\n",
        "    trainloader  = trainloader,\n",
        "    testloader   = testloader,\n",
        "    criterion    = criterion,\n",
        "    optimizer    = optimizer,\n",
        "    epochs       = cfg.epochs,\n",
        "    log_interval = cfg.log_interval,\n",
        "    device       = cfg.device\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05bace67",
      "metadata": {
        "id": "05bace67"
      },
      "outputs": [],
      "source": [
        "all_train_history = [train_history_cae_shallow]\n",
        "all_test_history_auroc_img = [test_history_auroc_img_cae_shallow]\n",
        "all_test_history_auroc_pixel = [test_history_auroc_pixel_cae_shallow]\n",
        "all_exp_name = ['CAE shallow']\n",
        "\n",
        "figure(\n",
        "    all_train_history            = all_train_history,\n",
        "    all_test_history_auroc_img   = all_test_history_auroc_img,\n",
        "    all_test_history_auroc_pixel = all_test_history_auroc_pixel,\n",
        "    all_exp_name                 = all_exp_name\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c49bf8ec",
      "metadata": {
        "id": "c49bf8ec"
      },
      "source": [
        "## CAE - deep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7dbf0b8f",
      "metadata": {
        "id": "7dbf0b8f"
      },
      "outputs": [],
      "source": [
        "torch_seed(cfg.seed)\n",
        "cae_deep = ConvolutionalAutoEncoder(input_dim=3, dims=[32, 64, 128, 256, 512])\n",
        "cae_deep.to(cfg.device)\n",
        "print('load Convolutional Auto-Encoder')\n",
        "print('The number of model parameters: ',sum([p.numel() for p in cae_deep.parameters()]))\n",
        "\n",
        "# set reduction to none\n",
        "criterion = nn.MSELoss(reduction='none')\n",
        "optimizer = Adam(cae_deep.parameters(), lr=cfg.learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6162f240",
      "metadata": {
        "scrolled": false,
        "id": "6162f240"
      },
      "outputs": [],
      "source": [
        "torch_seed(cfg.seed)\n",
        "train_history_cae_deep, test_history_auroc_img_cae_deep, test_history_auroc_pixel_cae_deep = fit(\n",
        "    model        = cae_deep,\n",
        "    trainloader  = trainloader,\n",
        "    testloader   = testloader,\n",
        "    criterion    = criterion,\n",
        "    optimizer    = optimizer,\n",
        "    epochs       = cfg.epochs,\n",
        "    log_interval = cfg.log_interval,\n",
        "    device       = cfg.device\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e1acee5",
      "metadata": {
        "id": "5e1acee5"
      },
      "outputs": [],
      "source": [
        "all_train_history.append(train_history_cae_deep)\n",
        "all_test_history_auroc_img.append(test_history_auroc_img_cae_deep)\n",
        "all_test_history_auroc_pixel.append(test_history_auroc_pixel_cae_deep)\n",
        "all_exp_name.append('CAE deep')\n",
        "\n",
        "figure(\n",
        "    all_train_history            = all_train_history,\n",
        "    all_test_history_auroc_img   = all_test_history_auroc_img,\n",
        "    all_test_history_auroc_pixel = all_test_history_auroc_pixel,\n",
        "    all_exp_name                 = all_exp_name\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7cda344f",
      "metadata": {
        "id": "7cda344f"
      },
      "source": [
        "# Experiment results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1242102",
      "metadata": {
        "id": "f1242102"
      },
      "outputs": [],
      "source": [
        "auroc_img_list = [\n",
        "    test_history_auroc_img_cae_shallow[-1],\n",
        "    test_history_auroc_img_cae_deep[-1]\n",
        "]\n",
        "\n",
        "auroc_pixel_list = [\n",
        "    test_history_auroc_pixel_cae_shallow[-1],\n",
        "    test_history_auroc_pixel_cae_deep[-1]\n",
        "]\n",
        "\n",
        "pd.DataFrame({\n",
        "    'Model'       : ['CAE shallow','CAE deep'],\n",
        "    'AUROC(image)': auroc_img_list,\n",
        "    'AUROC(pixel)': auroc_pixel_list\n",
        "}).round(4)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f45344aa",
      "metadata": {
        "id": "f45344aa"
      },
      "source": [
        "# Anomaly score distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa71d5da",
      "metadata": {
        "id": "aa71d5da"
      },
      "outputs": [],
      "source": [
        "# category file list\n",
        "file_list_cat = list(map(lambda x: x.split('/')[-2], testset.file_list))\n",
        "\n",
        "# loss function\n",
        "criterion = nn.MSELoss(reduction='none')\n",
        "\n",
        "# inference\n",
        "total_loss = []\n",
        "\n",
        "cae_deep.eval()\n",
        "with torch.no_grad():\n",
        "    for (inputs, _, _) in testloader:\n",
        "        inputs = inputs.to(cfg.device)\n",
        "        outputs = cae_deep(inputs)\n",
        "        loss = criterion(inputs, outputs)\n",
        "        total_loss.extend(loss.flatten(start_dim=1).max(dim=1)[0].cpu().tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b8799c8",
      "metadata": {
        "id": "1b8799c8"
      },
      "outputs": [],
      "source": [
        "sns.displot(\n",
        "    x      = total_loss,\n",
        "    hue    = file_list_cat,\n",
        "    kind   = 'kde',\n",
        "    fill   = True,\n",
        "    aspect = 2\n",
        ")\n",
        "plt.title('Anomaly score distribution')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea36e422",
      "metadata": {
        "id": "ea36e422"
      },
      "source": [
        "# Anomaly visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17dd89fa",
      "metadata": {
        "id": "17dd89fa"
      },
      "outputs": [],
      "source": [
        "# category file list\n",
        "file_list_cat = list(map(lambda x: x.split('/')[-2], testset.file_list))\n",
        "\n",
        "# set row name\n",
        "row_name = ['image', 'mask', 'anomaly region', 'image x anomaly']\n",
        "\n",
        "fig, ax = plt.subplots(4, len(testset.category), figsize=(2*len(testset.category), 8))\n",
        "\n",
        "# loss function\n",
        "criterion = nn.MSELoss(reduction='none')\n",
        "\n",
        "cae_deep.eval()\n",
        "for i, c in enumerate(testset.category):\n",
        "    # get index per category\n",
        "    idx = np.where(np.array(file_list_cat) == c)[0][2]\n",
        "\n",
        "    # get image and mask\n",
        "    img, mask, _ = testset[idx]\n",
        "\n",
        "    # get loss output\n",
        "    with torch.no_grad():\n",
        "        output = cae_deep(img.unsqueeze(0).to(cfg.device))[0].cpu()\n",
        "        loss = criterion(img, output).max(dim=0)[0]\n",
        "\n",
        "    # scaling\n",
        "    loss = (loss-loss.min()) / (loss.max()-loss.min())\n",
        "\n",
        "    # show image\n",
        "    ax[0, i].imshow(img.permute(1,2,0))\n",
        "    ax[1, i].imshow(mask, cmap='gray')\n",
        "    ax[2, i].imshow(loss, cmap='gray')\n",
        "    ax[3, i].imshow(img.permute(1,2,0)*0.5 + (loss*0.5).unsqueeze(-1))\n",
        "\n",
        "    # axis off\n",
        "    ax[0, i].axis('off')\n",
        "    ax[1, i].axis('off')\n",
        "    ax[2, i].axis('off')\n",
        "    ax[3, i].axis('off')\n",
        "\n",
        "    # set title\n",
        "    for r_idx, ax_r in enumerate(ax[:, i]):\n",
        "        ax_r.set_title(f'[{c}]\\n{row_name[r_idx]}' if r_idx==0 else row_name[r_idx])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R2QJrM_nHNmK"
      },
      "id": "R2QJrM_nHNmK",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}